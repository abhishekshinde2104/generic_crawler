# Website Measurement Crawler

This project automates browser-based website crawling to collect web request/response data, screenshots, and logs for measurement purposes. It supports multiple browsers (Firefox, Chrome, Brave, Edge) and works in headless or GUI modes.

## ğŸ“ Project Structure

```text
â”œâ”€â”€ cert_installation.py       # Certificate installation logic for SSL interception
â”œâ”€â”€ config.py                  # Stores summary data used during crawling
â”œâ”€â”€ crawl_logging.py           # Logging utility
â”œâ”€â”€ csv_storage.py             # Initializes and stores web data to CSV
â”œâ”€â”€ main.py                    # Main script to run the crawler
â”œâ”€â”€ setup_webdriver.py         # WebDriver setup for multiple browsers
â”œâ”€â”€ utils.py                   # Helpers for screenshots, URL parsing, etc.
â”œâ”€â”€ crawling_csv/
â”‚   â””â”€â”€ tranco_list.csv        # CSV file with list of websites to crawl
â”œâ”€â”€ drivers/
â”‚   â”œâ”€â”€ linux/
â”‚   â””â”€â”€ windows/               # Place browser drivers here
â”œâ”€â”€ measurements/              # Output directory created during each run
    â””â”€â”€ YYYY-MM-DD_HH-MM-SS/
        â”œâ”€â”€ session.csv          # Stores captured web request data
        â”œâ”€â”€ summary.txt          # Run summary
        â”œâ”€â”€ logfile.log          # Runtime logs
        â”œâ”€â”€ browser_profile/     # Browser data profile (optional)
        â””â”€â”€ website_screenshots/ # Screenshots for each website
```

## How to Run

### 1. Create and Activate Virtual Environment

```bash
# Create
python3 -m venv venv

# Activate
# On Linux/macOS:
source venv/bin/activate

# On Windows:
venv\Scripts\activate
```

---

### 2. Install Dependencies

```bash
pip install -r requirements.txt
```

---

### 3. Set Up Browser Drivers

Place the appropriate and compatible WebDriver executables in the `drivers/linux/` or `drivers/windows/` directory.

| Browser  | Driver            |
|----------|-------------------|
| Firefox  | `geckodriver`     |
| Chrome   | `chromedriver`    |
| Brave    | `chromedriver`    |
| Edge     | `msedgedriver`    |

---

### 4. Prepare Input CSV

Ensure `crawling_csv/tranco_list.csv` exists and contains:

```csv
1,example.com
2,wikipedia.org
3,github.com
```

---

### 5. Run the Crawler

```bash
# Example: Headless Firefox
python main.py --browser firefox --country india --headless

# Example: Chrome with GUI
python main.py --browser chrome --country usa
```

Valid `--browser` options: `firefox`, `chrome`, `brave`, `edge`  
`--country` specifies country (e.g., germany, india, usa, sweden)  
`--headless` makes the browser run invisibly

`--country` flag is present so that we can use it to differentiate results, in case the plan includes to use openvpn and do crawling in different country.

---

## ğŸ“Š Output Results

After each run, a folder is created in `measurements/`:

```text
â”œâ”€â”€ session.csv          # Stores captured web request data
â”œâ”€â”€ summary.txt          # Run summary
â”œâ”€â”€ logfile.log          # Runtime logs
â”œâ”€â”€ browser_profile/     # Browser data profile (optional)
â””â”€â”€ website_screenshots/ # Screenshots for each website & in case of chrome also stores chrome profile on linux machines.
```
---

## ğŸ” Certificates

Certificates are automatically:

- Installed to the system/user trust store using `cert_installation.py`
- Removed upon completion
- Required for HTTPS traffic inspection via Selenium Wire, else we observe `Not Secure` on the website
